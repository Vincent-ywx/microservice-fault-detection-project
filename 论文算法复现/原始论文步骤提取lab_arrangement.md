---

---

ğŸ§ª ä¸€ã€å®éªŒç›®æ ‡å›é¡¾
-----------

* æ¨¡æ‹Ÿå¾®æœåŠ¡ç³»ç»Ÿæ•…éšœï¼›

* åˆ©ç”¨ Prometheus + Grafana é‡‡é›†ä¸ƒç§æŒ‡æ ‡æ•°æ®ï¼›

* æ„å»º AutoMAP å¼‚å¸¸è¡Œä¸ºå›¾ï¼›

* æ‰§è¡Œæ ¹å› è¯Šæ–­ç®—æ³•ï¼›

* éªŒè¯ä¸åˆ†æè¯Šæ–­ç²¾åº¦ï¼Œå¤ç°è®ºæ–‡å®éªŒç»“è®ºã€‚

* * *

ğŸ—ï¸ äºŒã€åŸºç¡€å¹³å°æ­å»º
------------

### Step 1ï¼šéƒ¨ç½² Kubernetes é›†ç¾¤

å¯é€‰æ–¹æ¡ˆï¼š

* æœ¬åœ°ï¼šMinikube / Kindï¼›

* äº‘ç«¯ï¼šKubernetes on EKS / GKE / AKSï¼›

* æ¨èï¼šMinikube + Dockerï¼ˆå®éªŒæ€§ç¯å¢ƒè¶³å¤Ÿï¼‰ã€‚

```bash
minikube start --cpus=4 --memory=8192
```

* * *

### Step 2ï¼šéƒ¨ç½²å¾®æœåŠ¡ç³»ç»Ÿï¼ˆPymicroï¼‰

æ¨èå¤ç°è®ºæ–‡ä¸­ä½¿ç”¨çš„ [Pymicro](https://github.com/rshriram/pymicro) å¾®æœåŠ¡å¹³å°ï¼š

```bash
git clone https://github.com/rshriram/pymicro
cd pymicro
kubectl apply -f k8s/  # å‡è®¾æœ‰K8séƒ¨ç½²YAMLï¼ˆæˆ–å°†Docker Composeè½¬æ¢ï¼‰
```

ç¡®ä¿éƒ¨ç½²çš„æœåŠ¡ä¹‹é—´å­˜åœ¨å…¸å‹è°ƒç”¨å…³ç³»ï¼Œä¾‹å¦‚ frontend -> API -> databaseã€‚

åŒ…æ‹¬ 16 ä¸ª Docker å¾®æœåŠ¡ï¼Œç”± Zookeeper ç®¡ç†

* * *

### Step 3ï¼šå®‰è£… ChaosMeshï¼ˆæ•…éšœæ³¨å…¥å·¥å…·ï¼‰

```bash
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=chaos-testing --create-namespace --set chaosDaemon.runtime=containerd
```

> æ³¨æ„ï¼šéœ€å¯ç”¨ Webhook åŠæƒé™é…ç½®ï¼Œè¯¦è§ ChaosMesh å®˜æ–¹æ–‡æ¡£ã€‚

* * *

### Step 4ï¼šéƒ¨ç½² Prometheus + Grafana

ä½¿ç”¨ kube-prometheus-stack å®‰è£…ï¼š

```bash
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=chaos-testing --create-namespace --set chaosDaemon.runtime=containerd
```

ç¡®è®¤ä»¥ä¸‹ç»„ä»¶è¿è¡Œï¼š

* Prometheusï¼ˆæ”¶é›†æŒ‡æ ‡ï¼‰ï¼›

* Grafanaï¼ˆå±•ç¤ºæŒ‡æ ‡ï¼‰ï¼›

* Node exporter / cAdvisor / kube-state-metricsï¼ˆæ”¶é›†å®¿ä¸»æœºèµ„æºï¼‰ã€‚

* * *

### Step 5ï¼šé…ç½® Prometheus é‡‡é›†è‡ªå®šä¹‰æœåŠ¡æŒ‡æ ‡

å¦‚æœæœåŠ¡æœªè‡ªåŠ¨æš´éœ² `/metrics` æ¥å£ï¼š

* æ·»åŠ  Prometheus exporterï¼ˆå¦‚ `prometheus-client` Pythonï¼‰ï¼›

* æˆ–éƒ¨ç½² sidecar exporter å®¹å™¨ï¼›

* é…ç½® `ServiceMonitor` æ”¶é›†è§„åˆ™ã€‚

* * *

ğŸ› ä¸‰ã€æ•…éšœæ³¨å…¥å®éªŒè®¾è®¡ï¼ˆChaosMeshï¼‰
------------------------

### Step 6ï¼šå®šä¹‰æ³¨å…¥åœºæ™¯ï¼ˆæ¨¡æ‹Ÿè®ºæ–‡å®éªŒï¼‰

#### åœºæ™¯ Aï¼šå†…å­˜å ç”¨å¼‚å¸¸

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: stress-mem
  namespace: pymicro
spec:
  mode: one
  selector:
    labelSelectors:
      app: service-a
  stressors:
    memory:
      workers: 1
      size: 512Mi
  duration: "60s"

```

#### åœºæ™¯ Bï¼šæœåŠ¡ pod kill

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: kill-service-b
spec:
  action: pod-kill
  mode: one
  selector:
    namespaces: [pymicro]
    labelSelectors:
      app: service-b
  duration: "30s"
```



* * *

ğŸ“ˆ å››ã€æŒ‡æ ‡é‡‡é›†ä¸å¤„ç†
------------

### Step 7ï¼šé‡‡é›† 7 ç±»æŒ‡æ ‡

| æŒ‡æ ‡ä»£ç  | è¯´æ˜     | æ¥æºå»ºè®®                        |
| ---- | ------ | --------------------------- |
| Mlat | å»¶è¿Ÿ     | è‡ªå®šä¹‰ `/metrics` æ¥å£ã€HTTP è¯·æ±‚æ—¥å¿— |
| Mthr | åå     | è¯·æ±‚æ€»æ•° / ç§’                    |
| Mcon | æ‹¥å µåº¦    | Mthr / Mlat                 |
| Mcpu | CPUä½¿ç”¨ç‡ | node-exporter               |
| Mio  | IOæ“ä½œé¢‘ç‡ | node-exporter               |
| Mmem | å†…å­˜ä½¿ç”¨ç‡  | node-exporter               |
| Mavl | å¯ç”¨æ€§    | 200å“åº”ç‡                      |

* å¯ç”¨ Prometheus æŸ¥è¯¢è¯­è¨€ï¼ˆPromQLï¼‰æå–ï¼›

* ä½¿ç”¨ Python Prometheus HTTP API å¯¼å‡ºä¸º `.csv` æˆ– `np.array[n_services x t]`ã€‚

* * *

ğŸ¤– äº”ã€æ‰§è¡Œ AutoMAP ç®—æ³•æµç¨‹
--------------------

### Step 8ï¼šå‡†å¤‡æŒ‡æ ‡æ•°æ®

```python
metrics_dict = {
  'Mlat': np.array([...]),
  'Mthr': np.array([...]),
  ...
}
```

æ³¨æ„ï¼š

* æ¯ä¸ªæ•°ç»„å½¢çŠ¶åº”ä¸º `[æœåŠ¡æ•°é‡ Ã— æ—¶é—´ç‰‡]`ï¼›

* é‡‡æ ·å‘¨æœŸå»ºè®® 5 ç§’ã€‚

* * *

### Step 9ï¼šæ„å»ºå¼‚å¸¸è¡Œä¸ºå›¾

```python
G = build_behavior_graph(metrics_dict)
```

ä½¿ç”¨æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒï¼ˆå¦‚ Ï‡Â²ï¼‰å†³å®šæœåŠ¡é—´æ˜¯å¦å­˜åœ¨ä¾èµ–ã€‚

* * *

### Step 10ï¼šæ„å»ºæœåŠ¡ profile å’Œå¼‚å¸¸ profile

```python
profile = compute_profile(G)
anomaly_profile = current_profile - normal_profile
```

å¯ä¿å­˜æ­£å¸¸è¿è¡Œä¸‹çš„ profile å¹¶åœ¨æ•…éšœå‘ç”Ÿæ—¶å¯¹æ¯”ã€‚

* * *

### Step 11ï¼šç›¸ä¼¼æ€§åŒ¹é…ä¸è¯Šæ–­æŒ‡æ ‡é€‰æ‹©

```python
similar_profiles = find_most_similar_profiles(anomaly_profile, historical_db)
# è‡ªåŠ¨å­¦ä¹ æŒ‡æ ‡æƒé‡ w
```

* * *

### Step 12ï¼šå¯å‘å¼éšæœºæ¸¸èµ°å®šä½æ ¹å› 

```python
root_candidates = random_walk_root_cause(G, frontend_id)
top_k = root_candidates[:5]
```

* * *

âœ… å…­ã€ç»“æœè¯„ä¼°ä¸å¯è§†åŒ–
------------

### Step 13ï¼šè¯„ä¼°æŒ‡æ ‡è®¡ç®—

å¯¹æ¯”çœŸå®æ ¹å› æœåŠ¡ IDï¼š

* Top-1 / Top-3 / Top-5 ç²¾åº¦ï¼›

* avg-5 ç²¾åº¦ï¼›

* ç»˜åˆ¶æ··æ·†çŸ©é˜µã€PR æ›²çº¿ç­‰ã€‚

```python
precision = len(set(predicted[:k]) & set(actual_roots)) / len(actual_roots)
```

* * *

### Step 14ï¼šå¯è§†åŒ–è¾“å‡º

* Grafana ä»ªè¡¨ç›˜ç›‘æ§æ³¨å…¥å‰åçš„æŒ‡æ ‡å˜åŒ–ï¼›

* ä½¿ç”¨ `networkx` æˆ– `graphviz` ç»˜åˆ¶å¼‚å¸¸è¡Œä¸ºå›¾ï¼›

* è¾“å‡ºè¯Šæ–­è·¯å¾„ã€æ¸¸èµ°è½¨è¿¹ã€‚





ä¸€ä¸ªåŸºç¡€ç‰ˆçš„ AutoMAP ç®—æ³•å¤ç°ä»£ç ï¼ŒåŒ…å«ï¼š

1. **å¼‚å¸¸è¡Œä¸ºå›¾æ„å»º**ï¼ˆå¤šæŒ‡æ ‡ã€æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒï¼‰ï¼›

2. **æœåŠ¡ç‰¹å¾å‘é‡ï¼ˆProfileï¼‰ç”Ÿæˆ**ï¼›

3. **å¼‚å¸¸ç›¸ä¼¼åº¦åŒ¹é…**ï¼›

4. **å¯å‘å¼éšæœºæ¸¸èµ°æ ¹å› å®šä½ç®—æ³•**ã€‚

```python
# AutoMAPæ ¸å¿ƒé€»è¾‘ - Pythonä¼ªå®ç° (éœ€é…åˆPrometheusæŒ‡æ ‡æ•°æ®ä½¿ç”¨)
import numpy as np
import networkx as nx
from scipy.stats import chi2_contingency
from sklearn.preprocessing import normalize

# ========== é…ç½® ==========
ALPHA = 0.01  # æ¡ä»¶ç‹¬ç«‹æ€§æ˜¾è‘—æ€§æ°´å¹³

# ========== æ­¥éª¤1: æ„å»ºè¡Œä¸ºå›¾ ==========
def conditional_independence_test(data_i, data_j, data_cond):
    # ä½¿ç”¨å¡æ–¹æ£€éªŒè¿›è¡Œæ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯•ï¼ˆç®€åŒ–ç¤ºæ„ï¼‰
    contingency_table = np.histogram2d(data_i, data_j)[0]
    chi2, p_value, _, _ = chi2_contingency(contingency_table)
    return p_value > ALPHA

def build_behavior_graph(metrics_dict):
    # metrics_dict: {metric_name: np.array [n_services x time]}
    n_services = next(iter(metrics_dict.values())).shape[0]
    metrics = list(metrics_dict.keys())
    G = nx.DiGraph()

    for i in range(n_services):
        for j in range(n_services):
            if i == j:
                continue
            weights = []
            for metric in metrics:
                data = metrics_dict[metric]
                indep = conditional_independence_test(data[i], data[j], None)  # ç®€åŒ–
                weights.append(0. if indep else 1.)
            if sum(weights) > 0:
                norm_weights = np.array(weights) / np.count_nonzero(weights)
                G.add_edge(i, j, weight=norm_weights)
    return G

# ========== æ­¥éª¤2: è®¡ç®—æœåŠ¡ profile ==========
def compute_profile(G):
    profiles = {}
    for node in G.nodes():
        out_edges = G.out_edges(node, data=True)
        if not out_edges:
            profiles[node] = np.zeros(len(next(iter(out_edges), (None, None, {'weight': np.zeros(7)}))[2]['weight']))
            continue
        profile = np.mean([attr['weight'] for _, _, attr in out_edges], axis=0)
        profiles[node] = profile
    return profiles

# ========== æ­¥éª¤3: Profile ç›¸ä¼¼åº¦è®¡ç®— ==========
def cosine_similarity(a, b):
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)

def find_most_similar_profiles(target_profile, historical_profiles):
    scores = [(idx, cosine_similarity(target_profile, profile)) for idx, profile in historical_profiles.items()]
    return sorted(scores, key=lambda x: -x[1])

# ========== æ­¥éª¤4: éšæœºæ¸¸èµ°æ ¹å› è¯Šæ–­ ==========
def random_walk_root_cause(G, start_node, steps=1000, rho=0.2):
    visit_count = {node: 0 for node in G.nodes}
    current = start_node
    for _ in range(steps):
        neighbors = list(G.successors(current)) + list(G.predecessors(current)) + [current]
        probs = []
        for n in neighbors:
            if n == current:
                probs.append(0.2)  # self-transition
            elif G.has_edge(current, n):
                probs.append(0.7)
            else:
                probs.append(rho)  # backward
        probs = np.array(probs)
        probs /= probs.sum()
        current = np.random.choice(neighbors, p=probs)
        visit_count[current] += 1
    sorted_nodes = sorted(visit_count.items(), key=lambda x: -x[1])
    return sorted_nodes

# ========== ç”¨æ³•ç¤ºä¾‹ ==========
# 1. metrics_dict = {'Mlat': np.array([...]), 'Mcpu': np.array([...]), ...}
# 2. G = build_behavior_graph(metrics_dict)
# 3. profiles = compute_profile(G)
# 4. similar = find_most_similar_profiles(profiles[frontend_id], historical_profiles)
# 5. ranked_roots = random_walk_root_cause(G, frontend_id)

```
